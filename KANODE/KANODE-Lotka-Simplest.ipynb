{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that has the simplest implementation of KAN-ODEs. It shows how to generate the data, define a KAN then pass it to a NeuralODE, then shows how to train it. It also introduces the `ProgressBar.jl` package. There's no plotting or additional data saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module KolmogorovArnold.\n",
      "WARNING: using KolmogorovArnold.rbf in module Main conflicts with an existing identifier.\n",
      "WARNING: using KolmogorovArnold.KDense in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "activation_getter (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random, Lux, LinearAlgebra\n",
    "using NNlib, ConcreteStructs, WeightInitializers, ChainRulesCore\n",
    "using ComponentArrays\n",
    "using BenchmarkTools\n",
    "using OrdinaryDiffEq, DiffEqFlux, ForwardDiff\n",
    "using Flux: Adam, mae, update!\n",
    "using Flux\n",
    "using Optimisers\n",
    "using MAT\n",
    "using ProgressBars\n",
    "using Zygote: gradient as Zgrad\n",
    "\n",
    "# Load the KAN package from https://github.com/vpuri3/KolmogorovArnold.jl\n",
    "include(\"src/KolmogorovArnold.jl\")\n",
    "using .KolmogorovArnold\n",
    "#load the activation function getter (written for this project, see the corresponding script):\n",
    "include(\"Activation_getter.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the ODE and Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate data from the ODE \n",
    "\n",
    "$$ \\begin{cases}\n",
    "x' &= \\alpha x - \\beta x y \\\\\n",
    "y' &= \\gamma x y - \\delta y \\end{cases}$$\n",
    "\n",
    "Note we'll define the function using the inplace style as this saves allocating space in memory, speeding up the execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lotka! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function lotka!(du, u, p, t)\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α * u[1] - β * u[2] * u[1]\n",
    "    du[2] = γ * u[1] * u[2] - δ * u[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we simulate the ODE and generate the data by defining a object of type [ODEProblem](https://docs.sciml.ai/DiffEqDocs/stable/types/ode_types/) and calling [solve](https://docs.sciml.ai/DiffEqDocs/stable/basics/common_solver_opts/#CommonSolve.solve-Tuple%7BSciMLBase.AbstractDEProblem,%20Vararg%7BAny%7D%7D) which are part of the `DifferentialEquations.jl` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data generation parameters\n",
    "timestep=0.1\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)\n",
    "tspan = (0.0, 14)\n",
    "tspan_train=(0.0, 3.5)\n",
    "u0 = [1, 1]\n",
    "#Parameters of the ODE [α, β, γ, δ]\n",
    "p_ = Float32[1.5, 1, 1, 3]\n",
    "prob = ODEProblem(lotka!, u0, tspan, p_)\n",
    "\n",
    "#generate training data, split into train/test\n",
    "solution = solve(prob, Tsit5(), abstol = 1e-12, reltol = 1e-12, saveat = timestep)\n",
    "end_index=Int64(floor(length(solution.t)*tspan_train[2]/tspan[2]))\n",
    "t = solution.t #full dataset\n",
    "t_train=t[1:end_index] #training cut\n",
    "X = Array(solution)\n",
    "Xn = deepcopy(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the KAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the architecture of the KAN. Using the `KolmogorovArnold.jl` package, we can call Lux layers that are defined there to save us the work of defining them by ourselves. We may choose \n",
    "* which basis functions we would like (defined in `src/utils.jl`)\n",
    "* which normalizer functions we would like (defined in `src/utils.jl`)\n",
    "* the grid size the basis functions will use\n",
    "* the number of layers to our KAN\n",
    "* the width of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_func = rbf      # rbf, rswaf\n",
    "normalizer = tanh_fast # sigmoid(_fast), tanh(_fast), softsign \n",
    "grid_size=5 #Grid size for the activation functions \n",
    "##Not sure what this is? It seems like this normalizes the inputs \n",
    "##to be between -1,1,/0,1 but i dont quite see for sure where.\n",
    "num_layers=2 #number of layers in the KAN \n",
    "layer_width=10 #Width of each layer in the KAN (number of activation functions )\n",
    "\n",
    "kan1 = Lux.Chain(\n",
    "    KDense( 2, layer_width, grid_size; use_base_act = true, basis_func, normalizer),\n",
    "    KDense(layer_width,  2, grid_size; use_base_act = true, basis_func, normalizer),\n",
    ")\n",
    "pM , stM  = Lux.setup(rng, kan1) #Assign parameters and the state to memory\n",
    "\n",
    "\n",
    "pM_axis = getaxes(ComponentArray(pM))\n",
    "pM_data = getdata(ComponentArray(pM))\n",
    "p = (deepcopy(pM_data))./1e5 ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing to train\n",
    "\n",
    "## Construct a `NeuralODE` \n",
    "\n",
    "When we defined our KAN we constructed a model in Lux. With this type, we can easily pass it to the NeuralODE package for training. In order to train our model, we need to \n",
    "* Construct a `NeuralODE`\n",
    "* Define a loss function\n",
    "* Choose an optimizer\n",
    "* Choose an algorithm for constructing the gradient.\n",
    "\n",
    "In addition to training the model, we also want to see how well the model is predicting the future state of the ODE. To do this, we will instantiate a second `NeuralODE` called `train_node_test` which will have a timespan over the test set but will be built with the same `kan1`. This will allow us to easily see how our model performs over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_test (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_node      = NeuralODE(kan1, tspan_train, Tsit5(), saveat = t_train); \n",
    "train_node_test = NeuralODE(kan1, tspan, Tsit5(), saveat = t); #only difference is the time span\n",
    "\n",
    "function predict(p)\n",
    "    Array(train_node(u0, p, stM)[1])\n",
    "end\n",
    "function predict_test(p)\n",
    "    Array(train_node_test(u0, p, stM)[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function\n",
    "\n",
    "In order to train our model, we will need to specify a loss function. Probably the simplest loss function is the mean square error (MSE)\n",
    "$$\\mathcal{L}_1(\\theta) = MSE(u^{\\text{KAN}}(t, \\theta), u^{\\text{obs}}(t)) = \\frac{1}{N}\\sum_{i=1}^N \\lVert u^{\\text{KAN}}(t_i, \\theta) - u^{\\text{obs}}(t_i) \\rVert^2 $$ \n",
    "which are implemented as functions of the parameter `p` like so\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_test (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss_train(p)\n",
    "    mean(abs2, Xn[:, 1:end_index].- predict(ComponentArray(p,pM_axis)))\n",
    "end\n",
    "function loss_test(p)\n",
    "    mean(abs2, Xn .- predict_test(ComponentArray(p,pM_axis)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in KAN-ODEs paper, we may also want to include a term to encourage sparcity in our model. One way we can do this is to add an $l1$ norm term to our loss \n",
    "$$\\mathcal{L}_2(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\| u^{\\text{KAN}}(t_i, \\theta) - u^{\\text{obs}}(t_i) \\|^2  + \\gamma_{sp} | \\theta |_1 $$\n",
    "which introduces a sparcity hyperparameter $\\gamma_{sp}$ which we may control. We'll add the sparcity term as a function `reg_loss` and add it to the result of `loss_train` for our total loss function `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#regularization loss \n",
    "function reg_loss(p, reg=1.0)\n",
    "    l1_temp=(abs.(p))\n",
    "    activation_loss=sum(l1_temp)\n",
    "    activation_loss*reg\n",
    "end\n",
    "\n",
    "#overall loss is the sum of trhe training loss and the sparcity regularization loss\n",
    "function loss(p)\n",
    "    loss_MSE=mean(abs2, Xn[:, 1:end_index].- predict(ComponentArray(p,pM_axis)))\n",
    "    loss_MSE + reg_loss(p, 5e-4)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an Optimizer\n",
    "The `Flux.jl` has an assortment of optimizers to choose from. Here we will choose `Adam` with a learning rate of `5e-4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.0005, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = Flux.Adam(5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Gradient Algorithm\n",
    "\n",
    "We are also able to choose which algorithm we would like to use to calculate the gradient of our loss function. We'll use the `Zgrad` alogirthm and include this explicitly when we do the training loop. (TODO: Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "At this stage we have everything we need to start the training loop. \n",
    "\n",
    "We'll want to see how far along we are in our training algorithm, so we'll want to make use of the `ProgressBars.jl` package. Passing a range into `ProgressBar` creates an object with Julia can iterate over, which also includes some extra functionality. By default, a progress bar will now appear in our terminal showing how far along our for loop is, as well as the rate at which it is iterating. We can also include additional information such as the current loss with `set_description`.\n",
    "\n",
    "In the training loop, we only do 3 actions\n",
    "1. Calculate the gradient using `Zgrad`\n",
    "2. Update the parameters of the model\n",
    "3. Calculate the current loss and test loss and print them to the `ProgressBar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                              ┫ 0/100 [00:00<00:-1, -0s/it]\n",
      "Loss:2.98|Train_Loss:9.45| 3.0%┣▋                  ┫ 3/100 [00:00<00:03, 30it/s]\n",
      "Loss:2.97|Train_Loss:9.42| 6.0%┣█▏                 ┫ 6/100 [00:00<00:02, 39it/s]\n",
      "Loss:2.96|Train_Loss:9.42| 9.0%┣█▊                 ┫ 9/100 [00:00<00:02, 43it/s]\n",
      "Loss:2.94|Train_Loss:9.47| 12.0%┣██               ┫ 12/100 [00:00<00:02, 46it/s]\n",
      "Loss:2.93|Train_Loss:9.55| 15.0%┣██▌              ┫ 15/100 [00:00<00:02, 46it/s]\n",
      "Loss:2.92|Train_Loss:9.67| 18.0%┣███              ┫ 18/100 [00:00<00:02, 48it/s]\n",
      "Loss:2.90|Train_Loss:9.83| 21.0%┣███▋             ┫ 21/100 [00:00<00:02, 48it/s]\n",
      "Loss:2.89|Train_Loss:10.04| 24.0%┣███▉            ┫ 24/100 [00:00<00:02, 48it/s]\n",
      "Loss:2.87|Train_Loss:10.30| 27.0%┣████▎           ┫ 27/100 [00:01<00:01, 49it/s]\n",
      "Loss:2.86|Train_Loss:10.61| 30.0%┣████▉           ┫ 30/100 [00:01<00:01, 49it/s]\n",
      "Loss:2.84|Train_Loss:10.97| 33.0%┣█████▎          ┫ 33/100 [00:01<00:01, 50it/s]\n",
      "Loss:2.82|Train_Loss:11.39| 36.0%┣█████▊          ┫ 36/100 [00:01<00:01, 49it/s]\n",
      "Loss:2.81|Train_Loss:11.87| 39.0%┣██████▎         ┫ 39/100 [00:01<00:01, 50it/s]\n",
      "Loss:2.79|Train_Loss:12.41| 42.0%┣██████▊         ┫ 42/100 [00:01<00:01, 50it/s]\n",
      "Loss:2.77|Train_Loss:13.01| 45.0%┣███████▏        ┫ 45/100 [00:01<00:01, 50it/s]\n",
      "Loss:2.75|Train_Loss:13.67| 48.0%┣███████▊        ┫ 48/100 [00:01<00:01, 50it/s]\n",
      "Loss:2.73|Train_Loss:14.40| 51.0%┣████████▏       ┫ 51/100 [00:01<00:01, 50it/s]\n",
      "Loss:2.71|Train_Loss:15.20| 54.0%┣████████▋       ┫ 54/100 [00:01<00:01, 50it/s]\n",
      "Loss:2.69|Train_Loss:16.07| 57.0%┣█████████▏      ┫ 57/100 [00:01<00:01, 51it/s]\n",
      "Loss:2.67|Train_Loss:17.05| 60.0%┣█████████▋      ┫ 60/100 [00:01<00:01, 51it/s]\n",
      "Loss:2.65|Train_Loss:18.17| 63.0%┣██████████      ┫ 63/100 [00:01<00:01, 51it/s]\n",
      "Loss:2.63|Train_Loss:19.46| 66.0%┣██████████▋     ┫ 66/100 [00:01<00:01, 51it/s]\n",
      "Loss:2.61|Train_Loss:20.98| 69.0%┣███████████     ┫ 69/100 [00:01<00:01, 51it/s]\n",
      "Loss:2.59|Train_Loss:22.77| 72.0%┣███████████▌    ┫ 72/100 [00:01<00:01, 51it/s]\n",
      "Loss:2.56|Train_Loss:24.87| 75.0%┣████████████    ┫ 75/100 [00:01<00:00, 51it/s]\n",
      "Loss:2.54|Train_Loss:27.29| 78.0%┣████████████▌   ┫ 78/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.52|Train_Loss:30.06| 81.0%┣█████████████   ┫ 81/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.49|Train_Loss:33.24| 84.0%┣█████████████▍  ┫ 84/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.47|Train_Loss:36.86| 87.0%┣██████████████  ┫ 87/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.45|Train_Loss:40.96| 90.0%┣██████████████▍ ┫ 90/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.42|Train_Loss:45.60| 93.0%┣██████████████▉ ┫ 93/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.40|Train_Loss:50.81| 96.0%┣███████████████▍┫ 96/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.37|Train_Loss:56.64| 99.0%┣███████████████▉┫ 99/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.36|Train_Loss:58.73| 100.0%┣██████████████┫ 100/100 [00:02<00:00, 51it/s]\n",
      "Loss:2.36|Train_Loss:58.73| 100.0%┣██████████████┫ 100/100 [00:02<00:00, 51it/s]\n"
     ]
    }
   ],
   "source": [
    "using Printf\n",
    "# TRAINING\n",
    "N_iter = 100 #Number of training iterations\n",
    "\n",
    "iters=ProgressBar(1:N_iter)\n",
    "for i in iters\n",
    "    \n",
    "    # gradient computation\n",
    "    grad = Zgrad(loss, p)[1]\n",
    "\n",
    "    #model update\n",
    "    update!(optimizer, p, grad)\n",
    "\n",
    "    #loss metrics\n",
    "    loss_curr=deepcopy(loss_train(p))\n",
    "    loss_curr_test=deepcopy(loss_test(p))\n",
    "    set_description(iters, string(\"Loss:\", @sprintf(\"%.2f\", loss_curr), \"|\",\n",
    "                            \"Train_Loss:\", @sprintf(\"%.2f\", loss_curr_test), \"|\"))\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
