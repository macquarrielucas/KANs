{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Lux, LinearAlgebra\n",
    "using NNlib, ConcreteStructs, WeightInitializers, ChainRulesCore\n",
    "using ComponentArrays\n",
    "using BenchmarkTools\n",
    "using OrdinaryDiffEq, Plots, DiffEqFlux, ForwardDiff\n",
    "using Flux: Adam, mae, update!\n",
    "using Flux\n",
    "using Optimisers\n",
    "using MAT\n",
    "using Plots\n",
    "using ProgressBars\n",
    "using Zygote: gradient as Zgrad\n",
    "\n",
    "# Load the KAN package from https://github.com/vpuri3/KolmogorovArnold.jl\n",
    "include(\"src/KolmogorovArnold.jl\")\n",
    "using .KolmogorovArnold\n",
    "#load the activation function getter (written for this project, see the corresponding script):\n",
    "include(\"Activation_getter.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the ODE and Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate data from the ODE \n",
    "\n",
    "$$ \\begin{cases}\n",
    "x' &= \\\\\n",
    "y' &= \\end{cases}$$\n",
    "\n",
    "Note we'll define the function using the inplace style as this saves allocating space in memory, speeding up the execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka!(du, u, p, t)\n",
    "    α, β, γ, δ = p\n",
    "    du[1] = α * u[1] - β * u[2] * u[1]\n",
    "    du[2] = γ * u[1] * u[2] - δ * u[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we simulate the ODE and generate the data by defining a object of type [ODEProblem](https://docs.sciml.ai/DiffEqDocs/stable/types/ode_types/) and calling [solve](https://docs.sciml.ai/DiffEqDocs/stable/basics/common_solver_opts/#CommonSolve.solve-Tuple%7BSciMLBase.AbstractDEProblem,%20Vararg%7BAny%7D%7D) which are part of the `DifferentialEquations.jl` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data generation parameters\n",
    "timestep=0.1\n",
    "n_plot_save=1000\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)\n",
    "tspan = (0.0, 14)\n",
    "tspan_train=(0.0, 3.5)\n",
    "u0 = [1, 1]\n",
    "p_ = Float32[1.5, 1, 1, 3]\n",
    "prob = ODEProblem(lotka!, u0, tspan, p_)\n",
    "\n",
    "#generate training data, split into train/test\n",
    "solution = solve(prob, Tsit5(), abstol = 1e-12, reltol = 1e-12, saveat = timestep)\n",
    "end_index=Int64(floor(length(solution.t)*tspan_train[2]/tspan[2]))\n",
    "t = solution.t #full dataset\n",
    "t_train=t[1:end_index] #training cut\n",
    "#NOTE: What are these?\n",
    "X = Array(solution)\n",
    "Xn = deepcopy(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the KAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the architecture of the KAN. Using the `KolmogorovArnold.jl` package, we can call Lux layers that are defined there to save us the work of defining them by ourselves. We may choose the type of basis function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_func = rbf      # rbf, rswaf\n",
    "normalizer = tanh_fast # sigmoid(_fast), tanh(_fast), softsign \n",
    "##Not sure what this is? It seems like this normalizes the inputs \n",
    "##to be between -1,1,/0,1 but i dont quite see for sure where.\n",
    "\n",
    "\n",
    "###layer_width and grid_size can be modified here to replicate the testing in section A2 of the manuscript\n",
    "\n",
    "num_layers=2 #defined just to save into .mat for plotting\n",
    "layer_width=10\n",
    "grid_size=5\n",
    "kan1 = Lux.Chain(\n",
    "    KDense( 2, layer_width, grid_size; use_base_act = true, basis_func, normalizer),\n",
    "    KDense(layer_width,  2, grid_size; use_base_act = true, basis_func, normalizer),\n",
    ")\n",
    "pM , stM  = Lux.setup(rng, kan1)\n",
    "\n",
    "l = []\n",
    "l_test=[]\n",
    "p_list = []\n",
    "pM_axis = getaxes(ComponentArray(pM))\n",
    "pM_data = getdata(ComponentArray(pM))\n",
    "p = (deepcopy(pM_data))./1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural ODE\n",
    "\n",
    "Next we pass our KAN to initialize a neuralODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node = NeuralODE(kan1, tspan_train, Tsit5(), saveat = t_train); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define some helper functions which will simplify the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(p)\n",
    "    Array(train_node(u0, p, stM)[1])\n",
    "end\n",
    "\n",
    "#regularization loss (see Eq. 12 in manuscript )\n",
    "function reg_loss(p, act_reg=1.0, entropy_reg=1.0)\n",
    "    l1_temp=(abs.(p))\n",
    "    activation_loss=sum(l1_temp)\n",
    "    entropy_temp=l1_temp/activation_loss\n",
    "    entropy_loss=-sum(entropy_temp.*log.(entropy_temp))\n",
    "    total_reg_loss=activation_loss*act_reg+entropy_loss*entropy_reg\n",
    "    return total_reg_loss\n",
    "end\n",
    "\n",
    "#overall loss\n",
    "function loss(p)\n",
    "    loss_temp=mean(abs2, Xn[:, 1:end_index].- predict(ComponentArray(p,pM_axis)))\n",
    "    if sparse_on==1\n",
    "        loss_temp+=reg_loss(p, 5e-4, 0) #if we have sparsity enabled, add the reg loss\n",
    "    end\n",
    "    return loss_temp\n",
    "end\n",
    "\n",
    "function predict_test(p)\n",
    "    Array(train_node_test(u0, p, stM)[1])\n",
    "end\n",
    "\n",
    "function loss_train(p)\n",
    "    mean(abs2, Xn[:, 1:end_index].- predict(ComponentArray(p,pM_axis)))\n",
    "end\n",
    "function loss_test(p)\n",
    "    mean(abs2, Xn .- predict_test(ComponentArray(p,pM_axis)))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we are able to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "du = [0.0; 0.0]\n",
    "optimizer = Flux.Adam(5e-4)\n",
    "\n",
    "N_iter = 10\n",
    "i_current = 1\n",
    "\n",
    "##Actual training loop:\n",
    "iters=tqdm(1:N_iter-i_current)\n",
    " for i in iters\n",
    "    global i_current\n",
    "    \n",
    "    # gradient computation\n",
    "    grad = Zgrad(loss, p)[1]\n",
    "\n",
    "    #model update\n",
    "    update!(optimizer, p, grad)\n",
    "\n",
    "    #loss metrics\n",
    "    loss_curr=deepcopy(loss_train(p))\n",
    "    loss_curr_test=deepcopy(loss_test(p))\n",
    "    append!(l, [loss_curr])\n",
    "    append!(l_test, [loss_curr_test])\n",
    "    append!(p_list, [deepcopy(p)])\n",
    "\n",
    "    set_description(iters, string(\"Loss:\", loss_curr))\n",
    "    i_current = i_current + 1\n",
    "\n",
    "    #=\n",
    "    if i%n_plot_save==0\n",
    "        plot_save(l, l_test, p_list, i)\n",
    "    end\n",
    "    =#\n",
    "    \n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
